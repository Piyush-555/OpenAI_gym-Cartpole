{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Dependencies\n",
    "import gym\n",
    "import numpy as np\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from collections import Counter\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating data\n",
    "def generate_data(n_episodes):\n",
    "    accepted_scores=[]\n",
    "    train_data=[]\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        score=0\n",
    "        prev_observation = env.reset()\n",
    "        temp=[]\n",
    "        for timesteps in range(500):\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            temp.append([prev_observation,action])\n",
    "            prev_observation=observation\n",
    "            score+=reward\n",
    "            if done:\n",
    "                break\n",
    "        if score>=50:\n",
    "            accepted_scores.append(score)\n",
    "            for data in temp:\n",
    "                if data[1]==1:\n",
    "                    y=[0,1]\n",
    "                else:\n",
    "                    y=[1,0]\n",
    "                train_data.append([data[0],y])\n",
    "\n",
    "    train_data=np.array(train_data)\n",
    "    np.save('train.npy',train_data)\n",
    "    print(\"Average rewards:\",mean(accepted_scores))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "def neural_network(n_input):\n",
    "    net = tflearn.input_data(shape=[None, n_input, 1])\n",
    "    net = tflearn.fully_connected(net, 32, activation='relu')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 16, activation='relu')\n",
    "    net = tflearn.dropout(net, 0.8)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(train_data, n_input):\n",
    "    X=np.array([i[0] for i in train_data]).reshape(-1, 4, 1)\n",
    "    Y=[i[1] for i in train_data]\n",
    "    model.fit(X, Y, n_epoch=3, show_metric=True, run_id='first_ai_game')    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Environment and Defining the model\n",
    "env = gym.make('CartPole-v0')\n",
    "model=neural_network(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data\n",
    "data=generate_data(10000)\n",
    "print('Generating Data Completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training or Loading the model \n",
    "#model=train_model(data, 4)\n",
    "#print('Model Train Completed...')\n",
    "model.load('model_type2_197.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rendering the Environment\n",
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(10):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        if len(prev_obs)==0:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(model.predict(prev_obs.reshape(-1,4,1))[0])\n",
    "        choices.append(action)         \n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score+=reward\n",
    "        if done:\n",
    "            break\n",
    "    scores.append(score)\n",
    "    print('Episode {} Score: {}'.format(each_game+1,score))\n",
    "\n",
    "print('Average Score:',sum(scores)/len(scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
